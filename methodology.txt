To reproduce the authors’ results, we trained all the required models using the code provided in their repository. However, some modifications were necessary to align with our experimental needs. Although the authors mentioned that their experiments were done for multiple runs, their code did not include an option to set a specific seed. We addressed this by adding a command-line argument to control the random seed, ensuring reproducibility. Additionally, we introduced a feature that allows saving trained models with custom names for better organization. The authors had included their implementations of VGG16, ResNet50, and GoogLeNet. The code expected datasets to be organized as folders of images, so we created a script to convert CIFAR-10 and CIFAR-100 datasets into this format. TinyImageNet was downloaded in the required format directly. One important change involved modifying the VGG16 implementation, which was originally hardcoded to accept 32×32 images. While this worked for CIFAR datasets, it caused issues with TinyImageNet, which uses higher-resolution images. To resolve this, we added an adaptive pooling layer to VGG16, making it resolution-independent. ResNet50 and GoogLeNet did not require any such changes as they already used adaptive pooling. Finally, we trained each model using three different seed values for 200 epochs to ensure consistent and robust evaluation.

In accordance with the paper, 1% of the most confident predictions were selected as representative samples for each class. This corresponded to 50 samples per class for CIFAR-10, and 5 samples per class for both CIFAR-100 and TinyImageNet, given their larger class counts. An additional experiment was also conducted using 50 samples per class for CIFAR-100; however, it was observed that this did not significantly impact the final results, so the default of 5 samples per class was maintained for consistency.

To generate class-specific masks used in model disassembly, the selected samples are passed through the network to collect intermediate activations. The core logic of the experiment is implemented here: activations from specific layers are accumulated and analyzed to determine the importance of each feature/channel for a given class. Based on these activations, binary masks are generated using thresholding strategies controlled by parameters alpha and beta. These masks indicate which features are relevant and are later used for pruning or disassembling the model. The original implementation provided by the authors assumed that a folder named masks already existed, and it lacked flexibility in naming or organizing outputs. We modified the code to automatically create the mask folder if it doesn’t exist and to allow specifying a custom name for the folder where masks are saved. 
This is the most resource-intensive part of the experiment and can consume a significant amount of RAM — exceeding 50 GB in the case of ResNet50. To mitigate this, we modified the code to allow the batch size to be adjusted, as it was originally hardcoded to 256.

The disassembly step uses the masks generated in the previous stage to prune the model. It relies on a dependency graph to ensure structural consistency while removing unnecessary channels, both from intermediate layers and the final classification layer. In the original code provided by the authors, the part responsible for pruning the final classification layer was commented out by default. This meant that disassembled models retained the full output size of the base model, even when only a subset of classes was retained. This not only skewed the reported accuracy of disassembled models but also caused compatibility issues in later stages such as parameter scaling. To address this, we uncommented the final pruning logic and ensured that the disassembled model’s output dimension matched the number of selected classes. Additionally, we modified the code to allow specifying a custom filename for the saved disassembled model.

To ensure the assembled model functions properly, we apply a parameter scaling step, which adjusts the magnitude of parameters in the final layer. This process compensates for the difference in feature magnitudes across disassembled models from different source domains. While the original paper emphasizes the importance of balancing feature magnitudes—especially in the final fully connected layer—it also describes a theoretical formulation for scaling across layers. However, in practice, the authors’ code only applies this normalization to the last layer, and we followed the same approach.

Although the authors’ GitHub implementation suggested this step was optional, we found it to be essential: without scaling, assembled models perform very poorly or fail altogether. Therefore, we implemented parameter scaling to align the final output magnitudes across disassembled components. Our implementation calculates confidence scores for correctly predicted samples in each class and derives a scaling factor to rescale the weights and biases of the last layer accordingly.

The assembly process was performed using the original code without modification. In this step, two disassembled models are merged by aligning and concatenating their corresponding layers. Specifically, convolutional and linear layers are padded as needed to match input dimensions, and then their weights and outputs are concatenated. 

The evaluation of both disassembled and assembled models was performed using the testing code provided by the authors. However, the authors did not provide tools to generate subsets of the datasets or to merge those subsets for evaluation. Therefore, we implemented custom code to create class-specific dataset subsets (e.g., selecting specific label ranges from CIFAR-10, CIFAR-100, or Tiny-ImageNet) and to merge them when needed for testing assembled models. This step was essential to ensure that the evaluation matched the setup described in the paper.
